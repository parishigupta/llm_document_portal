{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4689ee4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70df70c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d8153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a99614e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.042393896728754044,\n",
       " -0.03852992132306099,\n",
       " -0.04543685540556908,\n",
       " -0.010400405153632164,\n",
       " 0.057711824774742126,\n",
       " 0.018246879801154137,\n",
       " 0.010995607823133469,\n",
       " -0.03159891068935394,\n",
       " -0.0061469534412026405,\n",
       " 0.08103373646736145]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embed = embedding_model.embed_query(\"what is the capital of India?\")\n",
    "query_embed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded7df48",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29c3799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a456451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(),\"data\",\"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31fb38f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "len(documents)  #each page represents one document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea5f7d",
   "metadata": {},
   "source": [
    "## Splitting the data : forming chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14ca655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len   #splits on the basis of characters, i.e. each chunk will be made up of 500 characters\n",
    ")\n",
    "splitted_documents = splitter.split_documents(documents)\n",
    "len(splitted_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c31d3431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvron∗ Louis Martin† Kevin Stone†\n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\LLM_Course_Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'} \n",
      "\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n"
     ]
    }
   ],
   "source": [
    "print(splitted_documents[0],\"\\n\")   #page_label = 1 --> actual page number is 1, page_index = 0\n",
    "print(splitted_documents[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6608361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\LLM_Course_Projects\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 76, 'page_label': '77'}, page_content='applications ofLlama 2, developers should perform safety testing and tuning tailored to their\\nspecific applications of the model. Please see the Responsible Use Guide available available at\\nhttps://ai.meta.com/llama/responsible-user-guide\\nTable 52: Model card forLlama 2.\\n77')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_documents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1066d8",
   "metadata": {},
   "source": [
    "## Embedding the chunks and storing it into vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb6e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(splitted_documents,embedding_model)\n",
    "\n",
    "#token->words\n",
    "# chunk--> it is collection of words(token)[characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6501939d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2openly to encourage responsible AI innovation. Based on our experience, an open approach draws\\nuponthecollectivewisdom,diversity,andingenuityoftheAI-practitionercommunitytorealizethebenefitsof\\nthis technology. Collaboration will make these models better and safer. The entire AI community—academic\\nresearchers, civil society, policymakers, and industry—must work together to rigorously analyze and expose'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = vector_store.similarity_search(\"What is llama2?\",k=2)  #By default it returns top 4 documents\n",
    "ans[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad32285",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd45a189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2openly to encourage responsible AI innovation. Based on our experience, an open approach draws\\nuponthecollectivewisdom,diversity,andingenuityoftheAI-practitionercommunitytorealizethebenefitsof\\nthis technology. Collaboration will make these models better and safer. The entire AI community—academic\\nresearchers, civil society, policymakers, and industry—must work together to rigorously analyze and expose'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\":7})\n",
    "ans2 = retriever.invoke(\"What is llama2?\")\n",
    "ans2[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de76dc",
   "metadata": {},
   "source": [
    "### Based on user question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63a1cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_temp = \"\"\"\n",
    "You're an expert in providing the answer's to the user question based on the context given to you.\n",
    "If you don't find the context prompt user with : \n",
    "\"I do not have the relevant information, please repharse or ask a different question.\"\n",
    "\n",
    "Context : {context}\n",
    "Question : {question}\n",
    "\n",
    "Answer : \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42cc25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_temp,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb3e39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parse = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a43152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51fa8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d1e11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\":retriever| format_docs, \"question\":RunnablePassthrough()}\n",
    "    | prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b30e8d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, so the user is asking about Llama 2's benchmarking experiments. I need to figure out how to answer this based on the provided context. Let me go through the context step by step.\\n\\nFirst, I see there are two main tables: Table 20 and Table 3. Table 20 shows performance on standard benchmarks for both Llama 1 and Llama 2, with different model sizes (7B, 13B, etc.). Each model has a series of numbers which likely represent different benchmark metrics. For example, Llama 2 70B has scores like 85.0, 82.8, etc. \\n\\nThen, Table 3 seems to focus on overall performance across grouped academic benchmarks, again comparing Llama 1 and Llama 2. The numbers here are also in a similar format, so they probably represent different metrics within academic settings.\\n\\nThere's also a mention of Human-Eval and MBPP code generation benchmarks. The context provides specific numbers for Llama 2 models of various sizes in these areas. For instance, the 70B model has a pass@1 of 29.9 and pass@100 of 89.0 on Human-Eval, which are pretty high.\\n\\nAnother part talks about World Knowledge, evaluated on NaturalQuestions and TriviaQA, though the specific results aren't in the provided context. It does mention that results are in Table 22, which isn't included here.\\n\\nAdditionally, the context mentions that the benchmarks are grouped into categories, and individual results are in Section A.2.2, which we don't have access to. Safety benchmarks are in Section 4.1, but again, not provided.\\n\\nSo, putting this together, the benchmarking experiments for Llama 2 cover several areas: standard benchmarks, code generation, and world knowledge. Each of these has specific tables with detailed metrics across different model sizes.\\n\\nThe user probably wants a summary of where Llama 2 stands in these benchmarks, maybe to compare it with other models or to understand its capabilities. They might be a researcher or someone interested in AI model performance.\\n\\nI should structure the answer by breaking down each category (standard, code generation, world knowledge), mention the specific benchmarks, and highlight the performance metrics for each model size. Also, note that more detailed information is available elsewhere, as some tables aren't included here.\\n\\nI should avoid technical jargon where possible and make it clear and concise. Make sure to mention that Llama 2 generally shows strong performance across the board, improving with larger model sizes.\\n</think>\\n\\nLlama 2's benchmarking experiments involve evaluating its performance across various tasks and benchmarks, including standard benchmarks, code generation, and world knowledge tasks. Here's a breakdown of the key experiments and results:\\n\\n### 1. **Standard Benchmarks (Table 20)**\\n   - **Llama 2 Models**: The performance of Llama 2 models (7B, 13B, 34B, 70B) is evaluated on standard benchmarks. The results show consistent improvement as the model size increases.\\n     - **70B Model**: Achieves the highest scores, with metrics such as 85.0, 82.8, 50.7, 85.3, 80.2, 80.2, 57.4, 60.2, 78.5, and 68.9.\\n     - **34B Model**: Scores include 83.7, 81.9, 50.9, 83.3, 76.7, 79.4, 54.5, 58.2, 74.3, and 62.6.\\n\\n### 2. **Code Generation Benchmarks (Human-Eval and MBPP)**\\n   - **Human-Eval**: Llama 2 models are compared with other open-source models on the Human-Eval benchmark.\\n     - **70B Model**: Achieves a pass@1 score of 29.9 and pass@100 score of 89.0.\\n     - **34B Model**: Scores include pass@1 of 22.6 and pass@100 of 77.2.\\n   - **MBPP (Massive Multitask Benchmark of Prompting)**: Llama 2 also performs well on this benchmark.\\n     - **70B Model**: pass@80 score of 81.4.\\n     - **34B Model**: pass@80 score of 76.1.\\n\\n### 3. **World Knowledge Benchmarks**\\n   - Llama 2 is evaluated on world knowledge tasks using the NaturalQuestions and TriviaQA benchmarks. While specific results are not provided in the context, these benchmarks are part of the overall evaluation framework.\\n\\n### 4. **Overall Performance (Table 3)**\\n   - Llama 2 models are compared with other open-source models on a suite of academic benchmarks. The results are grouped into categories, and the 70B model shows strong performance across all metrics.\\n\\n### Key Takeaways:\\n   - Llama 2 demonstrates strong performance across standard benchmarks, code generation, and world knowledge tasks.\\n   - Larger models (e.g., 70B) consistently outperform smaller ones, showing scalability in performance.\\n   - The benchmarks highlight Llama 2's capabilities in code generation and real-world knowledge tasks.\\n\\nFor more detailed results, refer to the specific tables mentioned in the context, such as Table 20 for standard benchmarks and Table 22 for world knowledge tasks.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what's llama2 bench marking experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0f3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187ba52b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dfce6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c5b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aa8a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675b0ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aecd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e723dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814a2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
